# -*- coding: utf-8 -*-
"""Dual-Output LSTM with Anomaly Weighted Loss_1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dCCoXVeu5qR789IHXVhnp__AcbNMUt5U
"""

# KObert 및 관련 라이브러리 설치
!pip install transformers tensorflow torch
# 추가적으로 필요한 라이브러리 설치 (pandas, numpy, scikit-learn 등)
!pip install pandas numpy scikit-learn

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("skt/kobert-base-v1")

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("monologg/kobert")

!pip install sentencepiece
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("skt/kobert-base-v1")

# ============================== #
# 1) 환경 설정 및 라이브러리 로드
# ============================== #
!pip -q install transformers==4.44.2 sentencepiece

import pandas as pd
import numpy as np
import random
import tensorflow as tf
import re

from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from transformers import AutoTokenizer, TFBertModel
from tensorflow.keras.layers import Input, Dense, Dropout, Layer
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import Model as KerasModel # Keras Model 클래스를 명확히 import


# ---- 하이퍼파라미터 ----
MAX_LEN = 128
EMBEDDING_DIM = 768
DROPOUT_RATE = 0.1
BATCH_SIZE = 32
EPOCHS = 5
LEARNING_RATE = 5e-5
ANOMALY_WEIGHT_LAMBDA = 1.5
RANDOM_SEED = 42

# 재현성
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

print("환경 설정 및 하이퍼파라미터 초기화 완료.")

# ============================== #
# 2) 데이터 로드 및 모의 라벨링
# ============================== #
file_path = '/content/yogiyo_reviews_30000.csv'
try:
    df = pd.read_csv(file_path)
    df = df[['content', 'score']].dropna()
    print(f"원본 데이터 로드 완료. 샘플 수: {len(df)}")
except FileNotFoundError:
    print(f"에러: 파일을 찾을 수 없습니다. {file_path}를 Colab에 업로드했는지 확인하세요.")
    print("샘플 데이터를 생성하여 진행합니다.")
    data = {
        'content': [f'샘플 리뷰 {i}입니다. 배달이 늦어 불만입니다.' if i % 5 == 0 else f'샘플 리뷰 {i}입니다.' for i in range(100)],
        'score': np.random.randint(1, 6, 100)
    }
    df = pd.DataFrame(data)
    df = df[['content', 'score']].dropna()

# ---- 감성 라벨링: 1~2=Neg, 3=Neu, 4~5=Pos ----
def label_sentiment(score):
    if score <= 2:
        return 'Negative'
    elif score == 3:
        return 'Neutral'
    else:
        return 'Positive'

df['sentiment_label'] = df['score'].apply(label_sentiment)

# ---- 요구사항 라벨 모의 생성 ----
REQUIREMENT_CATEGORIES = ['Delivery', 'UI/UX', 'Service', 'Price', 'Packaging']
def mock_label_requirements(text):
    keywords = {
        'Delivery': ['배달', '시간', '기사님', '지연', '늦어', '빨라'],
        'UI/UX': ['앱', '오류', '버그', '멈춰', '느려', '업데이트', '결제'],
        'Service': ['상담원', '고객센터', '응대', '친절', '불친절', '취소'],
        'Price': ['할인', '쿠폰', '배달비', '가격', '비싸', '요기패스'],
        'Packaging': ['포장', '새다', '흘러', '꼼꼼']
    }
    assigned = []
    text_lower = str(text).lower()
    for cat, kws in keywords.items():
        if any(kw in text_lower for kw in kws):
            assigned.append(cat)
    if not assigned:
        return random.choice(REQUIREMENT_CATEGORIES)
    return assigned[0]

df['requirement_label'] = df['content'].apply(mock_label_requirements)

# ---- 라벨 인코딩 ----
sentiment_map = {'Negative': 0, 'Neutral': 1, 'Positive': 2}
requirement_map = {cat: i for i, cat in enumerate(REQUIREMENT_CATEGORIES)}

df['sentiment_encoded']   = df['sentiment_label'].map(sentiment_map)
df['requirement_encoded'] = df['requirement_label'].map(requirement_map)

print("데이터 로드 및 모의 라벨링 완료.")
display(df.head())

# ================================== #
# 3) KoBERT 토크나이저 인코딩 함수
# ================================== #
# 중요: skt/kobert-base-v1는 SentencePiece 기반 → AutoTokenizer 사용
ckpt = "skt/kobert-base-v1"
tokenizer = AutoTokenizer.from_pretrained(ckpt, use_fast=False)

def encode_texts(tokenizer, texts, max_len):
    # padding 방식 변경: pad_to_max_length (deprecated) → padding="max_length"
    enc = tokenizer(
        texts,
        padding="max_length",
        truncation=True,
        max_length=max_len,
        return_tensors="tf"
    )
    return enc["input_ids"], enc["attention_mask"]

X_input_ids, X_attention_masks = encode_texts(tokenizer, df['content'].tolist(), MAX_LEN)
print(f"\n인코딩된 입력 (input_ids) 형태: {X_input_ids.shape}")
print(f"인코딩된 입력 (attention_masks) 형태: {X_attention_masks.shape}")

# ================================== #
# 4) 레이블/마스크 및 데이터 분할
# ================================== #
Y_sentiment   = to_categorical(df['sentiment_encoded'].values,   num_classes=len(sentiment_map))
Y_requirement = to_categorical(df['requirement_encoded'].values, num_classes=len(requirement_map))

# 이상치 마스크: Negative(0)일 때 1, 아니면 0
Y_anomaly_mask = np.where(df['sentiment_encoded'].values == 0, 1.0, 0.0)
print(f"\n전체 리뷰 중 Negative(Anomaly) 비율: {Y_anomaly_mask.mean():.2f}")

# Convert TensorFlow tensors to NumPy arrays before splitting
X_input_ids_np = X_input_ids.numpy()
X_attention_masks_np = X_attention_masks.numpy()

X_train_ids, X_test_ids, X_train_masks, X_test_masks, \
YS_train, YS_test, YR_train, YR_test, YM_train, YM_test = train_test_split(
    X_input_ids_np, X_attention_masks_np, Y_sentiment, Y_requirement, Y_anomaly_mask,
    test_size=0.2, random_state=RANDOM_SEED
)

print(f"\n학습 데이터 샘플 수: {len(X_train_ids)}, 검증 데이터 샘플 수: {len(X_test_ids)}")
print("데이터 로드, 전처리 및 분할 완료. KObert 모델 구축 준비 완료.")

# ================================== #
# 5) 모델 정의 (TFBertModel + 2헤드 - Subclassing)
# ================================== #
class KobertDualOutputModel(KerasModel):
    def __init__(self, bert_model_name, num_sentiment_classes, num_requirement_classes, dropout_rate, **kwargs):
        super().__init__(**kwargs)
        # 주의: KoBERT는 TF 가중치가 없으므로 from_pt=True 필요
        self.bert = TFBertModel.from_pretrained(bert_model_name, from_pt=True)
        self.dropout = Dropout(dropout_rate)
        self.sentiment_classifier = Dense(num_sentiment_classes, activation='softmax', name="sentiment")
        self.requirement_classifier = Dense(num_requirement_classes, activation='softmax', name="requirement")

    def call(self, inputs, training=False):
        # inputs는 딕셔너리 형태를 예상: {'input_ids': ..., 'attention_mask': ...}
        input_ids = inputs['input_ids']
        attention_mask = inputs['attention_mask']

        # BERT 모델 통과
        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, training=training)

        # 보편적으로 [CLS] 토큰 벡터 사용 (first token)
        pooled_output = bert_outputs.last_hidden_state[:, 0, :]

        # Dropout 적용
        x = self.dropout(pooled_output, training=training)

        # 분류 헤드 통과
        sentiment_logits = self.sentiment_classifier(x)
        requirement_logits = self.requirement_classifier(x)

        return {"sentiment": sentiment_logits, "requirement": requirement_logits}

# 모델 인스턴스 생성
model = KobertDualOutputModel(
    bert_model_name=ckpt,
    num_sentiment_classes=len(sentiment_map),
    num_requirement_classes=len(requirement_map),
    dropout_rate=DROPOUT_RATE
)

# ================================== #
# 6) 손실/옵티마이저/지표 설정
# ================================== #
# 기본 크로스엔트로피
losses = {
    "sentiment":   tf.keras.losses.CategoricalCrossentropy(),
    "requirement": tf.keras.losses.CategoricalCrossentropy()
}

metrics = {
    "sentiment":   [tf.keras.metrics.CategoricalAccuracy(name="acc")],
    "requirement": [tf.keras.metrics.CategoricalAccuracy(name="acc")]
}

optimizer = Adam(learning_rate=LEARNING_RATE)

# Subclassing 모델은 build() 메서드를 호출하여 입력 형태를 명시해주거나, 첫 번째 fit/evaluate 호출 시 자동 빌드됨
# 명시적으로 build 해주는 것이 좋습니다.
model.build(input_shape={"input_ids": (None, MAX_LEN), "attention_mask": (None, MAX_LEN)})


model.compile(optimizer=optimizer, loss=losses, metrics=metrics)
model.summary()

# ================================== #
# 7) 샘플 가중치(이상치 가중) 설정
# ================================== #
# requirement 쪽에만 이상치 가중치 적용:
# sample_weight_req = 1 + (lambda-1) * anomaly_mask
sample_weight_req_train = 1.0 + (ANOMALY_WEIGHT_LAMBDA - 1.0) * YM_train
sample_weight_req_test  = 1.0 + (ANOMALY_WEIGHT_LAMBDA - 1.0) * YM_test

# sentiment 쪽은 균등 가중치(전부 1.0)
sample_weight_sent_train = np.ones_like(YM_train, dtype=np.float32)
sample_weight_sent_test  = np.ones_like(YM_test, dtype=np.float32)

# Keras는 출력별 sample_weight를 dict로 받는다.
train_sample_weights = {
    "sentiment":   sample_weight_sent_train,
    "requirement": sample_weight_req_train
}
val_sample_weights = {
    "sentiment":   sample_weight_sent_test,
    "requirement": sample_weight_req_test
}

# ================================== #
# 8) 학습
# ================================== #
print("\n모델 학습 시작...")
history = model.fit(
    x={"input_ids": X_train_ids, "attention_mask": X_train_masks},
    y={"sentiment": YS_train, "requirement": YR_train},
    sample_weight=train_sample_weights,
    validation_data=(
        {"input_ids": X_test_ids, "attention_mask": X_test_masks},
        {"sentiment": YS_test, "requirement": YR_test},
        val_sample_weights
    ),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE
)

print("\n모델 학습 완료.")

# ================================== #
# 9) 평가 및 간단 예측 예시
# ================================== #
eval_result = model.evaluate(
    x={"input_ids": X_test_ids, "attention_mask": X_test_masks},
    y={"sentiment": YS_test, "requirement": YR_test},
    sample_weight=val_sample_weights,
    batch_size=BATCH_SIZE,
    return_dict=True
)
print("\n[평가 결과]")
for k, v in eval_result.items():
    print(f"{k}: {v:.4f}")

# 임의 문장 예측
sample_texts = [
    "배달이 너무 늦어서 화가 났습니다.",
    "앱 결제가 자꾸 오류가 나요.",
    "가격도 괜찮고 포장도 깔끔했어요."
]
sample_ids, sample_masks = encode_texts(tokenizer, sample_texts, MAX_LEN)
pred = model.predict({"input_ids": sample_ids, "attention_mask": sample_masks})

inv_sent = {v:k for k,v in sentiment_map.items()}
inv_req  = {v:k for k,v in requirement_map.items()}

print("\n[샘플 예측]")
for t, ps, pr in zip(sample_texts, pred["sentiment"], pred["requirement"]):
    s_cls = inv_sent[int(np.argmax(ps))]
    r_cls = inv_req[int(np.argmax(pr))]
    print(f"- \"{t}\": Sentiment={s_cls}, Requirement={r_cls}")

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, TFBertModel # KObert를 위한 import
import random
import tensorflow as tf

# 하이퍼파라미터 설정 (논문 3.5.2절 기반 - KObert에 맞게 일부 조정)
MAX_LEN = 128         # KObert는 일반적으로 더 긴 시퀀스 길이를 사용
EMBEDDING_DIM = 768   # KObert base 모델의 임베딩 차원
DROPOUT_RATE = 0.1    # KObert fine-tuning 시 흔히 사용되는 dropout 비율
BATCH_SIZE = 32       # GPU 메모리 고려하여 batch size 조정
EPOCHS = 5            # Fine-tuning 시에는 적은 epoch으로 충분할 수 있습니다.
LEARNING_RATE = 5e-5  # KObert fine-tuning을 위한 학습률
ANOMALY_WEIGHT_LAMBDA = 1.5 # 이상치 가중치 (논문 3.4.2절 기반)
RANDOM_SEED = 42

# 재현성 확보를 위한 시드 설정
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)
print("환경 설정 및 하이퍼파라미터 초기화 완료.")

# 2. 데이터 로드 및 모의 라벨링 (Mock Labeling)
# 원본 CSV 파일 경로: 'yogiyo_reviews_30000.csv'
file_path = '/content/yogiyo_reviews_30000.csv'
try:
    df = pd.read_csv(file_path)
    df = df[['content', 'score']].dropna() # 리뷰 내용과 평점만 사용
    print(f"원본 데이터 로드 완료. 샘플 수: {len(df)}")
except FileNotFoundError:
    print(f"에러: 파일을 찾을 수 없습니다. {file_path}를 Colab에 업로드했는지 확인하세요.")
    # 데이터 파일을 찾을 수 없을 경우 샘플 데이터를 생성하여 진행
    print("샘플 데이터를 생성하여 진행합니다.")
    data = {'content': [f'샘플 리뷰 {i}입니다.' for i in range(100)], 'score': np.random.randint(1, 6, 100)}
    df = pd.DataFrame(data)
    df = df[['content', 'score']].dropna()


# ----------------------------------------------------------------------
# 2.1 감성 분석 라벨링 (Sentiment Labeling)
# 논문 3.1.1절 기반: 평점을 기준으로 라벨링
# Negative: 1~2점, Neutral: 3점, Positive: 4~5점
# ----------------------------------------------------------------------
def label_sentiment(score):
    if score <= 2:
        return 'Negative'  # 0
    elif score == 3:
        return 'Neutral'   # 1
    else:
        return 'Positive'  # 2

df['sentiment_label'] = df['score'].apply(label_sentiment)

# ----------------------------------------------------------------------
# 2.2 요구사항 라벨링 (Requirement Labeling) - 모의 (Mock) 구현
# 실제 논문에서는 수작업 라벨링이 필요합니다. 여기서는 키워드를 기반으로 모의 라벨을 부여합니다.
# 요구사항 카테고리: Delivery, UI/UX, Service, Price, Packaging
# ----------------------------------------------------------------------
REQUIREMENT_CATEGORIES = ['Delivery', 'UI/UX', 'Service', 'Price', 'Packaging']

def mock_label_requirements(text):
    # 키워드 기반 모의 라벨링 딕셔너리
    keywords = {
        'Delivery': ['배달', '시간', '기사님', '지연', '늦어', '빨라'],
        'UI/UX': ['앱', '오류', '버그', '멈춰', '느려', '업데이트', '결제'],
        'Service': ['상담원', '고객센터', '응대', '친절', '불친절', '취소'],
        'Price': ['할인', '쿠폰', '배달비', '가격', '비싸', '요기패스'],
        'Packaging': ['포장', '새다', '흘러', '꼼꼼']
    }

    assigned_labels = []
    text_lower = str(text).lower() # Ensure text is string

    for category, kws in keywords.items():
        if any(kw in text_lower for kw in kws):
            assigned_labels.append(category)

    # 할당된 라벨이 없는 경우 가장 흔한 카테고리 중 하나를 랜덤으로 할당 (노이즈 방지)
    if not assigned_labels:
        return random.choice(REQUIREMENT_CATEGORIES)

    # 여러 라벨이 할당될 경우 첫 번째 라벨만 사용 (단일 출력 가정)
    return assigned_labels[0]

df['requirement_label'] = df['content'].apply(mock_label_requirements)

# 라벨 인코딩
sentiment_map = {'Negative': 0, 'Neutral': 1, 'Positive': 2}
requirement_map = {cat: i for i, cat in enumerate(REQUIREMENT_CATEGORIES)}

df['sentiment_encoded'] = df['sentiment_label'].map(sentiment_map)
df['requirement_encoded'] = df['requirement_label'].map(requirement_map)

print("데이터 로드 및 모의 라벨링 완료.")
display(df.head())

# 3. 데이터 전처리: KObert 토큰화 및 인코딩
# KObert 토크나이저 로드
# 'skt/kobert-base-v1'은 사전 학습된 KObert 모델의 식별자입니다.
tokenizer = BertTokenizer.from_pretrained('skt/kobert-base-v1')

def encode_texts(tokenizer, texts, max_len):
    input_ids = []
    attention_masks = []

    for text in texts:
        encoded = tokenizer.encode_plus(
            text,
            add_special_tokens=True, # [CLS], [SEP] 토큰 추가
            max_length=max_len,      # 최대 길이
            pad_to_max_length=True,  # 최대 길이까지 패딩
            return_attention_mask=True, # 어텐션 마스크 생성
            return_tensors='tf',     # TensorFlow 텐서 반환
            truncation=True          # 최대 길이 초과 시 자르기
        )
        input_ids.append(encoded['input_ids'])
        attention_masks.append(encoded['attention_mask'])

    return tf.concat(input_ids, axis=0), tf.concat(attention_masks, axis=0)

# 리뷰 내용을 KObert 토크나이저로 인코딩
X_input_ids, X_attention_masks = encode_texts(tokenizer, df['content'].tolist(), MAX_LEN)

print(f"\n인코딩된 입력 데이터 형태 (input_ids): {X_input_ids.shape}")
print(f"인코딩된 입력 데이터 형태 (attention_masks): {X_attention_masks.shape}")


# 출력 라벨 (Y) 준비
Y_sentiment = to_categorical(df['sentiment_encoded'].values, num_classes=len(sentiment_map))
Y_requirement = to_categorical(df['requirement_encoded'].values, num_classes=len(requirement_map))

# 이상치(Anomaly) 마스크 생성: Negative Sentiment (Sentiment Class 0)인 경우 1, 아니면 0
# Anomaly Mask는 Requirement Loss에만 적용됩니다.
Y_anomaly_mask = np.where(df['sentiment_encoded'].values == 0, 1.0, 0.0)
print(f"\n전체 리뷰 중 Negative(Anomaly) 리뷰 비율: {Y_anomaly_mask.sum() / len(Y_anomaly_mask):.2f}")


# 4. 데이터 분할 (Train/Test Split)
# KObert 입력 형태에 맞게 데이터 분할: input_ids, attention_masks 사용
X_train_ids, X_test_ids, X_train_masks, X_test_masks, YS_train, YS_test, YR_train, YR_test, YM_train, YM_test = train_test_split(
    X_input_ids, X_attention_masks, Y_sentiment, Y_requirement, Y_anomaly_mask,
    test_size=0.2, random_state=RANDOM_SEED
)

print(f"\n학습 데이터 샘플 수: {len(X_train_ids)}, 검증 데이터 샘플 수: {len(X_test_ids)}")

print("\n데이터 로드, 전처리 및 분할 완료. KObert 모델 구축 준비 완료.")