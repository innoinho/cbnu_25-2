# -*- coding: utf-8 -*-
"""Dual-Output LSTM with Anomaly Weighted Loss

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IeX-Psm-2RikAKh1zNT_nl8TTu9PQzCd
"""

# Commented out IPython magic to ensure Python compatibility.
# 1) 기존 MeCab 설치 흔적 제거 (선택사항, 충돌 방지)
!pip uninstall -y mecab-python3 konlpy

# 2) 필수 패키지 설치 (Google Colab용)
# apt-get으로는 mecab-ko 및 mecab-ko-dic 패키지를 찾을 수 없어 Colab에서 통상적으로 사용하는 설치 방법 사용
!git clone https://github.com/SOMJANG/Mecab-Ko-for-Google-Colab.git
# %cd Mecab-Ko-for-Google-Colab
!bash install_mecab-ko_on_colab_light_220429.sh

# 3) Python wrapper 설치
!pip install konlpy
!pip install mecab-python3

# 4) 설치 확인
from konlpy.tag import Mecab
mecab = Mecab()
print("✅ Mecab 정상 작동:", mecab.pos("형태소 분석이 제대로 작동합니다."))

# Commented out IPython magic to ensure Python compatibility.
# -*- coding: utf-8 -*-
# ----------------------------------------------------------------------
# Ⅲ. 제안 모델 및 연구 방법 구현 (수정판)
# Dual-Head LSTM + Anomaly-Weighted Sample Weights (요구사항 헤드에만 가중치)
# ----------------------------------------------------------------------

# 0) Colab 환경 준비 (필요 시)
!pip -q install konlpy yake
# apt-get으로는 mecab-ko 및 mecab-ko-dic 패키지를 찾을 수 없어 Colab에서 통상적으로 사용하는 설치 방법 사용
!git clone https://github.com/SOMJANG/Mecab-Ko-for-Google-Colab.git
# %cd Mecab-Ko-for-Google-Colab
!bash install_mecab-ko_on_colab_light_220429.sh


import os, re, random
import numpy as np
import pandas as pd
import tensorflow as tf
from konlpy.tag import Mecab

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical

import yake

# -----------------------------
# 하이퍼파라미터
# -----------------------------
MAX_LEN = 100
EMBEDDING_DIM = 200
LSTM_UNITS = 128
DROPOUT_RATE = 0.2
BATCH_SIZE = 64
EPOCHS = 10
LEARNING_RATE = 0.001
ANOMALY_WEIGHT_LAMBDA = 1.5  # Negative에 대한 요구사항 가중치
RANDOM_SEED = 42

# 재현성
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

print("환경 설정 및 하이퍼파라미터 초기화 완료.")

# -----------------------------
# 1) 데이터 로드
# -----------------------------
file_path = '/content/yogiyo_reviews_30000.csv'
try:
    df = pd.read_csv(file_path)
    df = df[['content', 'score']].dropna()
    print(f"원본 데이터 로드 완료. 샘플 수: {len(df)}")
except FileNotFoundError:
    raise FileNotFoundError(f"에러: 파일을 찾을 수 없습니다. {file_path}를 Colab에 업로드하세요.")

# -----------------------------
# 2) 라벨링
# -----------------------------
def label_sentiment(score):
    if score <= 2:   return 'Negative'
    elif score == 3: return 'Neutral'
    else:            return 'Positive'

df['sentiment_label'] = df['score'].apply(label_sentiment)

REQUIREMENT_CATEGORIES = ['Delivery', 'UI/UX', 'Service', 'Price', 'Packaging']

def mock_label_requirements(text):
    keywords = {
        'Delivery':   ['배달','시간','기사','지연','늦어','빨라'],
        'UI/UX':      ['앱','오류','버그','멈춰','느려','업데이트','결제'],
        'Service':    ['상담원','고객센터','응대','친절','불친절','취소'],
        'Price':      ['할인','쿠폰','배달비','가격','비싸','요기패스'],
        'Packaging':  ['포장','새다','흘러','꼼꼼']
    }
    t = str(text)
    hits = []
    for cat, kws in keywords.items():
        if any(kw in t for kw in kws):
            hits.append(cat)
    if not hits:
        return random.choice(REQUIREMENT_CATEGORIES)
    return hits[0]

df['requirement_label'] = df['content'].apply(mock_label_requirements)

sentiment_map = {'Negative':0, 'Neutral':1, 'Positive':2}
requirement_map = {c:i for i,c in enumerate(REQUIREMENT_CATEGORIES)}

df['sentiment_encoded']   = df['sentiment_label'].map(sentiment_map)
df['requirement_encoded'] = df['requirement_label'].map(requirement_map)

# 이상치 마스크(부정=1, 그 외=0) — 요구사항 헤드의 sample_weight에 사용
df['anomaly_mask'] = (df['sentiment_encoded'] == 0).astype(float)
print(f"Negative 비율(=이상치 마스크 1): {df['anomaly_mask'].mean():.2f}")

# -----------------------------
# 3) 전처리 (Mecab 토큰화)
# -----------------------------
mecab = Mecab()

def mecab_tokenizer(text):
    text = re.sub(r'[^가-힣A-Za-z0-9\s]', ' ', str(text))
    pos = mecab.pos(text)
    # 명사(NNG, NNP), 동사(VV/VX), 형용사(VA) 중심
    keep = []
    for tok, tag in pos:
        if tag in ('NNG','NNP','VV','VA','VX','MM'):
            keep.append(tok)
    return keep

df['tokenized'] = df['content'].apply(mecab_tokenizer)
texts = df['tokenized'].apply(lambda x: ' '.join(x)).tolist()

tokenizer = Tokenizer(oov_token='<OOV>')
tokenizer.fit_on_texts(texts)
VOCAB_SIZE = len(tokenizer.word_index) + 1
print(f"단어장 크기: {VOCAB_SIZE}")

seqs = tokenizer.texts_to_sequences(texts)
X = pad_sequences(seqs, maxlen=MAX_LEN, padding='post', truncating='post')

Y_sentiment   = to_categorical(df['sentiment_encoded'].values, num_classes=len(sentiment_map))
Y_requirement = to_categorical(df['requirement_encoded'].values, num_classes=len(requirement_map))
Y_mask        = df['anomaly_mask'].values.astype('float32')

# -----------------------------
# 4) 데이터 분할
# -----------------------------
X_train, X_test, YS_tr, YS_te, YR_tr, YR_te, M_tr, M_te = train_test_split(
    X, Y_sentiment, Y_requirement, Y_mask, test_size=0.2, random_state=RANDOM_SEED
)

print(f"학습/검증: {len(X_train)}/{len(X_test)}")

# -----------------------------
# 5) 모델 구성 (두 개의 출력)
# -----------------------------
inp = Input(shape=(MAX_LEN,), name='input_ids')
emb = Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LEN, name='emb')(inp)
x = LSTM(LSTM_UNITS, dropout=DROPOUT_RATE, name='shared_lstm')(emb)

# Removed name attribute from Dense layers
sentiment_out   = Dense(len(sentiment_map),   activation='softmax')(x)
requirement_out = Dense(len(requirement_map), activation='softmax')(x)

model = Model(inputs=inp, outputs=[sentiment_out, requirement_out])

model.compile(
    optimizer=Adam(learning_rate=LEARNING_RATE),
    loss=['categorical_crossentropy', 'categorical_crossentropy'], # Loss as list
    metrics=[['accuracy'], ['accuracy']] # Metrics as list of lists
)

model.summary()
print("모델 컴파일 완료.")

# -----------------------------
# 6) 이상치 가중치: sample_weight로 적용
#    요구사항(head)에만 (1 + (λ-1)*mask)
# -----------------------------
sw_sent_train = np.ones((len(X_train),), dtype='float32')
sw_req_train  = 1.0 + (ANOMALY_WEIGHT_LAMBDA - 1.0) * M_tr

sw_sent_val = np.ones((len(X_test),), dtype='float32')
sw_req_val  = 1.0 + (ANOMALY_WEIGHT_LAMBDA - 1.0) * M_te

print("모델 학습 시작...")
history = model.fit(
    X_train,
    [YS_tr, YR_tr], # Pass y as a list
    sample_weight=[sw_sent_train, sw_req_train], # Pass sample_weight as a list
    validation_data=(X_test,
                     [YS_te, YR_te], # Pass validation y as a list
                     [sw_sent_val, sw_req_val]), # Pass validation sample_weight as a list
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    verbose=1
)
print("모델 학습 완료.")

# -----------------------------
# 7) 평가 (정확도, Weighted F1)
# -----------------------------
YS_pred, YR_pred = model.predict(X_test, batch_size=BATCH_SIZE, verbose=0)

ys_true = np.argmax(YS_te, axis=1)
ys_pred = np.argmax(YS_pred, axis=1)
yr_true = np.argmax(YR_te, axis=1)
yr_pred = np.argmax(YR_pred, axis=1)

acc_sent = accuracy_score(ys_true, ys_pred)
f1_sent  = f1_score(ys_true, ys_pred, average='weighted')
acc_req  = accuracy_score(yr_true, yr_pred)
f1_req   = f1_score(yr_true, yr_pred, average='weighted')

print("\n-------------------------------------------")
print("          모델 성능 평가 결과 (Weighted F1)")
print("-------------------------------------------")
print(f"1. 감성 분석 (Sentiment) Accuracy: {acc_sent:.4f}")
print(f"   감성 분석 (Sentiment) F1-Score: {f1_sent:.4f}")
print(f"2. 요구사항 분류 (Requirement) Accuracy: {acc_req:.4f}")
print(f"   요구사항 분류 (Requirement) F1-Score: {f1_req:.4f}")
print("-------------------------------------------")
print("※ 이상치(부정 리뷰)에서 요구사항 손실에만 λ 가중치를 부여했습니다.")

# -----------------------------
# 8) YAKE 기반 키워드 추출 (부정 리뷰)
# -----------------------------
# Colab에서 YAKE 사용을 위해 추가 설치
!pip install yake

negative_reviews = df[df['sentiment_label']=='Negative']['content'].astype(str).tolist()
full_negative_text = ' '.join(negative_reviews)

kw_extractor = yake.KeywordExtractor(lan="ko", n=2, top=20, features=None)
keywords = kw_extractor.extract_keywords(full_negative_text)

print("\n-------------------------------------------")
print("YAKE 기반 핵심 요구사항 키워드 추출 결과")
print("-------------------------------------------")
for kw, score in keywords:
    print(f"키워드: {kw} (점수: {score:.4f})")
print("※ 4.5절에서 워드클라우드/빈도 그래프로 시각화해 해석하세요.")

# -*- coding: utf-8 -*-
# 4.5절 시각화: 워드클라우드 생성

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# YAKE에서 추출한 키워드 (점수 포함)
# keywords 변수는 ra-SiHM0EC8D 셀에서 생성되었습니다.
# 예시: [('요기요', 0.0002), ('요기패스', 0.0002), ...]

# 워드클라우드 생성을 위해 키워드와 점수를 딕셔너리 형태로 변환
# YAKE 점수는 낮을수록 중요도가 높으므로, 1/score 또는 (max_score - score) 등으로 중요도를 역전시킬 수 있습니다.
# 여기서는 간단히 키워드 자체를 사용하거나, 점수를 역전시켜 빈도로 활용하는 방식을 고려할 수 있습니다.
# YAKE 점수 자체가 빈도를 나타내는 것은 아니므로, 여기서는 키워드만 추출하여 빈도를 계산하거나, 점수를 가중치로 사용합니다.
# 가장 간단한 방법은 추출된 키워드를 텍스트로 변환 후 빈도 기반 워드클라우드를 생성하는 것입니다.

# 방법 1: 추출된 키워드 리스트에서 키워드만 추출하여 공백으로 연결 (빈도 기반)
keywords_list = [kw for kw, score in keywords]
text_for_wordcloud = ' '.join(keywords_list)

# Colab 한글 폰트 설정
!apt-get update -qq
!apt-get install fonts-nanum -qq > /dev/null

import matplotlib.font_manager as fm
import os

fe = fm.FontEntry(
    fname='/usr/share/fonts/truetype/nanum/NanumGothic.ttf', # NanumGothic.ttf
    name='NanumGothic')
fm.fontManager.ttflist.insert(0, fe)
plt.rcParams.update({'font.size': 10, 'font.family': 'NanumGothic'})


# WordCloud 객체 생성
wordcloud = WordCloud(
    font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf', # 한글 폰트 경로 지정
    width=800,
    height=400,
    background_color='white'
).generate(text_for_wordcloud)

# 워드클라우드 시각화
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('부정 리뷰 주요 키워드 워드클라우드')
plt.show()

print("✅ 워드클라우드 시각화 완료")

# 방법 2: YAKE 점수를 가중치로 사용하는 워드클라우드 (선택 사항)
# YAKE 점수는 낮을수록 중요도가 높으므로, (1 / score) 또는 (최대점수 - score) 등으로 변환하여 사용
# 여기서는 YAKE 점수가 매우 작으므로, 1/score 방식을 사용해 봅니다. (0점 방지 위해 작은 값 더함)
# score가 0인 경우는 없지만, 혹시 몰라 1e-6을 더해줍니다.
keywords_dict = {kw: 1/(score + 1e-6) for kw, score in keywords}

wordcloud_weighted = WordCloud(
    font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf', # 한글 폰트 경로 지정
    width=800,
    height=400,
    background_color='white'
).generate_from_frequencies(keywords_dict)

# 가중치 기반 워드클라우드 시각화
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_weighted, interpolation='bilinear')
plt.axis('off')
plt.title('부정 리뷰 주요 키워드 워드클라우드 (YAKE 점수 가중치 반영)')
plt.show()

print("✅ 가중치 반영 워드클라우드 시각화 완료")

# -*- coding: utf-8 -*-
# 4.5절 시각화: 키워드 빈도 그래프 생성

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# YAKE에서 추출한 키워드 (점수 포함)
# keywords 변수는 ra-SiHM0EC8D 셀에서 생성되었습니다.
# 예시: [('요기요', 0.0002), ('요기패스', 0.0002), ...]

# 키워드와 점수를 DataFrame으로 변환하여 시각화 준비
keywords_df = pd.DataFrame(keywords, columns=['Keyword', 'YAKE_Score'])

# YAKE 점수는 낮을수록 중요도가 높으므로, 점수가 낮은(중요한) 키워드 순으로 정렬
keywords_df = keywords_df.sort_values(by='YAKE_Score', ascending=True).reset_index(drop=True)

# 상위 N개 키워드 선택 (예: 상위 20개)
top_n_keywords = keywords_df.head(20)

# Colab 한글 폰트 설정 (워드클라우드 셀에서 이미 설치했다면 생략 가능)
# !apt-get update -qq
# !apt-get install fonts-nanum -qq > /dev/null
# import matplotlib.font_manager as fm
# import os
# fe = fm.FontEntry(fname='/usr/share/fonts/truetype/nanum/NanumGothic.ttf', name='NanumGothic')
# fm.fontManager.ttflist.insert(0, fe)
# plt.rcParams.update({'font.size': 10, 'font.family': 'NanumGothic'})


# 키워드 빈도(또는 YAKE 점수 역순) 그래프 시각화
plt.figure(figsize=(12, 8))
# YAKE 점수 자체가 빈도가 아니므로, 시각화를 위해 YAKE 점수 역순으로 정렬한 결과를 막대 그래프로 표시
# 실제 빈도를 사용하려면 원본 텍스트에서 토큰 빈도를 계산해야 합니다.
# 여기서는 YAKE 점수(낮을수록 중요)를 Y축에 표시하고 키워드 중요도를 간접적으로 나타냅니다.

sns.barplot(x='YAKE_Score', y='Keyword', data=top_n_keywords, palette='viridis')
plt.title('부정 리뷰 주요 키워드 중요도 (YAKE Score)')
plt.xlabel('YAKE Score (낮을수록 중요)')
plt.ylabel('키워드')
plt.gca().invert_xaxis() # YAKE Score는 낮을수록 중요하므로 X축 반전
plt.show()

print("✅ 키워드 중요도(YAKE Score) 그래프 시각화 완료")

# 만약 실제 키워드 빈도 그래프를 원하신다면, 원본 부정 리뷰 텍스트에서 Mecab 토큰화 후 빈도를 계산해야 합니다.
# 아래는 참고용 코드입니다. (위의 YAKE 키워드와는 다를 수 있습니다)
"""
from collections import Counter

negative_reviews_text = ' '.join(df[df['sentiment_label'] == 'Negative']['tokenized'].apply(lambda x: ' '.join(x)).tolist())
tokens_in_negative_reviews = negative_reviews_text.split()

# 빈도 계산
word_counts = Counter(tokens_in_negative_reviews)

# 가장 흔한 단어 상위 N개
top_common_words = word_counts.most_common(20)

top_common_words_df = pd.DataFrame(top_common_words, columns=['Keyword', 'Frequency'])

plt.figure(figsize=(12, 8))
sns.barplot(x='Frequency', y='Keyword', data=top_common_words_df, palette='viridis')
plt.title('부정 리뷰 토큰 빈도 상위 20개')
plt.xlabel('빈도')
plt.ylabel('키워드')
plt.show()

print("✅ 부정 리뷰 토큰 빈도 그래프 시각화 완료 (참고용)")
"""